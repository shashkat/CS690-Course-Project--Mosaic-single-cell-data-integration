"""cs690_course_project_compact_2.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d_Oh0ukb69SaVF1HAIteGGs3KZnQzJqV
"""

### imports and installs

# import sys
import warnings
warnings.filterwarnings("ignore")
import anndata as ad
import numpy as np
import pandas as pd
import scanpy as sc
import scvi
import seaborn as sn
from scipy.sparse import csr_matrix
from scvi.model.utils import mde
# import pymde


# branch = "stable"
# IN_COLAB = "google.colab" in sys.modules

# if IN_COLAB and branch == "stable":
#     !pip install --quiet scvi-tools[tutorials]
# elif IN_COLAB and branch != "stable":
#     !pip install --quiet --upgrade jsonschema
#     !pip install --quiet git+https://github.com/yoseflab/scvi-tools@$branch#egg=scvi-tools[tutorials]


adata_raw = []
adata_raw.append(sc.read_10x_h5('./data/connect_5k_pbmc_NGSC3_ch5_filtered_feature_bc_matrix.h5'))
adata_raw.append(sc.read_10x_h5('./data/pbmc_10k_v3_filtered_feature_bc_matrix.h5'))

for i in range(len(adata_raw)):
  adata_raw[i].var_names_make_unique()

adata_raw[0] = adata_raw[0][:,:25000]

adata_raw[1] = adata_raw[1][:,5000:30000]

#### first dataset reading
print("Reading dataset...")

temp = pd.concat([adata_raw[0].obs, adata_raw[1].obs])
dup_inds = np.nonzero(temp.index.duplicated())

dup_inds1 = []
for i in range(len(adata_raw[0].obs)):
   dup_inds1.append(i in dup_inds[0])

dup_inds2 = []
for i in range(len(adata_raw[0].obs),len(temp)):
   dup_inds2.append(i in dup_inds[0])

if len(dup_inds1) != 0: adata1_nondup = adata_raw[0][~np.array(dup_inds1)]
if len(dup_inds2) != 0: adata2_nondup = adata_raw[1][~np.array(dup_inds2)]

temp2 = pd.concat([adata1_nondup.obs, adata2_nondup.obs])
temp2[temp2.index.duplicated()]

adata = ad.concat([adata1_nondup,adata2_nondup], label = 'batch')

adata.layers["counts"] = adata.X.copy()
sc.pp.normalize_total(adata, target_sum=1e4)
sc.pp.log1p(adata)
adata.raw = adata  # keep full dimension safe

##second methodolgy

###clustering
print("Clustering...")

sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)
sc.pl.highly_variable_genes(adata)
adata.raw = adata
adata = adata[:, adata.var.highly_variable]
sc.pp.scale(adata, max_value=10)

sc.tl.pca(adata, svd_solver='arpack')
sc.pl.pca_variance_ratio(adata, log=True)
sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)

sc.tl.umap(adata)
sc.tl.leiden(adata)

sc.pl.umap(adata, color=['leiden'])

### clustering for each dataset individually
print("Clustering for each dataset individually...")

adata_raw[0].raw = adata_raw[0]

sc.pp.normalize_total(adata_raw[0], target_sum=1e4)
sc.pp.log1p(adata_raw[0])

sc.pp.highly_variable_genes(adata_raw[0], min_mean=0.0125, max_mean=3, min_disp=0.5)
sc.pl.highly_variable_genes(adata_raw[0])
adata_raw[0] = adata_raw[0][:, adata_raw[0].var.highly_variable]
sc.pp.scale(adata_raw[0], max_value=10)

sc.tl.pca(adata_raw[0], svd_solver='arpack')
sc.pl.pca_variance_ratio(adata_raw[0], log=True)
sc.pp.neighbors(adata_raw[0], n_neighbors=80, n_pcs=40)

sc.tl.umap(adata_raw[0])
sc.tl.leiden(adata_raw[0])
sc.pl.umap(adata_raw[0], color=['leiden'])

adata_raw[1].raw = adata_raw[1]

sc.pp.normalize_total(adata_raw[1], target_sum=1e4)
sc.pp.log1p(adata_raw[1])

sc.pp.highly_variable_genes(adata_raw[1], min_mean=0.0125, max_mean=3, min_disp=0.5)
sc.pl.highly_variable_genes(adata_raw[1])
adata_raw[1] = adata_raw[1][:, adata_raw[1].var.highly_variable]
sc.pp.scale(adata_raw[1], max_value=10)

sc.tl.pca(adata_raw[1], svd_solver='arpack')
sc.pl.pca_variance_ratio(adata_raw[1], log=True)
sc.pp.neighbors(adata_raw[1], n_neighbors=100, n_pcs=40)

sc.tl.umap(adata_raw[1])
sc.tl.leiden(adata_raw[1])
sc.pl.umap(adata_raw[1], color=['leiden'])

### finding distance between clusters
print("Finding distance between clusters...")

adata_raw_orig = []
adata_raw_orig.append(adata_raw[0].raw.to_adata())
adata_raw_orig.append(adata_raw[1].raw.to_adata())

adata_raw_orig[0][:,adata_raw_orig[0].var_names.intersection(adata_raw_orig[1].var_names)]

fes = np.array([[1,2],[5,3]])
np.mean(fes, axis = 0)

# constructing distance matrix
d1 = adata_raw_orig[0][:,adata_raw_orig[0].var_names.intersection(adata_raw_orig[1].var_names)]
d2 = adata_raw_orig[1][:,adata_raw_orig[0].var_names.intersection(adata_raw_orig[1].var_names)]

clust_dist_mat = [[0]*len(set(adata_raw[0].obs['leiden'])) for i in range(len(set(adata_raw[1].obs['leiden'])))]
for cluster1 in range(len(set(adata_raw[0].obs['leiden']))):
 for cluster2 in range(len(set(adata_raw[1].obs['leiden']))):
  arr1 = np.array(d1[d1.obs['leiden']==str(cluster1)].X.todense())
  arr2 = np.array(d2[d2.obs['leiden']==str(cluster2)].X.todense())
  x = np.mean(arr1)
  y = np.mean(arr2)
  clust_dist_mat[cluster2][cluster1] = x-y

clust_dist_mat = np.array(clust_dist_mat)

print(f"np.amin(clust_dist_mat) = {np.amin(clust_dist_mat)}")
print(f"np.mean(clust_dist_mat), np.var(clust_dist_mat) = {np.mean(clust_dist_mat)}, {np.var(clust_dist_mat)}")

clust_dist_mat_heatmap = np.absolute(clust_dist_mat)
sn.heatmap(clust_dist_mat_heatmap)

### filling unknown info using clusters distance matrix
print("Filling unknown info using clusters distance matrix...")

adata_union = ad.concat([adata_raw_orig[0], adata_raw_orig[1]], join = 'outer')

all_genes = set(adata_union.var_names)
adata1_genes = set(adata_raw_orig[0].var_names)
adata2_genes = set(adata_raw_orig[1].var_names)
sub2_set = all_genes-adata1_genes
sub1_set = all_genes-adata2_genes
sub2 = list(sub2_set)
sub1 = list(sub1_set)
sub1_adata = adata_union[:len(adata_raw_orig[0].obs), sub1]
sub1_adata_empty = adata_union[len(adata_raw_orig[0].obs):, sub1]

sub2_adata = adata_union[len(adata_raw_orig[0].obs):, sub2]
sub2_adata_empty = adata_union[:len(adata_raw_orig[0].obs), sub2]

for i in range(len(clust_dist_mat)):
  min = 1000000
  mini = 0
  minj = 0
  for j in range(len(clust_dist_mat[0])):
    if abs(clust_dist_mat[i][j]) < abs(min): 
      min = clust_dist_mat[i][j]
      mini = i
      minj = j
  what_to_fill = np.array(sub1_adata[sub1_adata.obs['leiden'] == str(minj)].X.todense())
  what_to_fill = np.mean(what_to_fill, axis = 0)
  what_to_fill -= min
  what_to_fill[what_to_fill<0] = 0
  what_to_fill = csr_matrix(what_to_fill)
  ct = 0
  cluster_subset = sub1_adata_empty[sub1_adata_empty.obs['leiden'] == str(i)]
  row = []
  for ind in cluster_subset.obs.index:
    ct += 1
    row.append(list(sub1_adata_empty.obs_names).index(ind))
  sub1_adata_empty.X[row] = what_to_fill

for i in range(len(clust_dist_mat[0])):
  min = 1000000
  mini = 0
  minj = 0
  for j in range(len(clust_dist_mat)):
    if abs(clust_dist_mat[j][i]) < abs(min): 
      min = clust_dist_mat[j][i]
      mini = i
      minj = j
  what_to_fill = np.array(sub2_adata[sub2_adata.obs['leiden'] == str(minj)].X.todense())
  what_to_fill = np.mean(what_to_fill, axis = 0)
  what_to_fill -= min
  what_to_fill[what_to_fill<0] = 0
  what_to_fill = csr_matrix(what_to_fill)
  ct = 0
  cluster_subset = sub2_adata_empty[sub2_adata_empty.obs['leiden'] == str(i)]
  row = []
  for ind in cluster_subset.obs.index:
    ct += 1
    row.append(list(sub2_adata_empty.obs_names).index(ind))
  sub2_adata_empty.X[row] = what_to_fill

common_genes = list(all_genes-sub1_set-sub2_set)
common_adata1 = adata_union[:len(adata_raw_orig[0].obs), common_genes]
common_adata2 = adata_union[len(adata_raw_orig[0].obs):, common_genes]

adata1 = ad.concat([common_adata1, sub1_adata, sub2_adata_empty], axis = 1)
adata2 = ad.concat([common_adata2, sub1_adata_empty, sub2_adata], axis = 1)

adata_full = ad.concat([adata1, adata2], label='batch')

### filled info verification using the reference data
print("Filled info verification using the reference data...")

adata_raw_veri = []
adata_raw_veri.append(sc.read_10x_h5('./data/connect_5k_pbmc_NGSC3_ch5_filtered_feature_bc_matrix.h5'))
adata_raw_veri.append(sc.read_10x_h5('./data/pbmc_10k_v3_filtered_feature_bc_matrix.h5'))

for i in range(len(adata_raw_veri)):
  adata_raw_veri[i].var_names_make_unique()

adata_raw_veri[0] = adata_raw_veri[0][:,25000:30000]

adata_raw_veri[1] = adata_raw_veri[1][:,0:5000]

# np1 = np.array(adata_raw_veri[0].X.todense())
diff1 = adata_raw_veri[0].X.todense() - sub2_adata_empty.X.todense()
diff1 = np.absolute(diff1)
diff1_mean = np.mean(diff1)

diff2 = adata_raw_veri[1].X.todense() - sub1_adata_empty.X.todense()
diff2 = np.absolute(diff2)
diff2_mean = np.mean(diff2)

print(f"abs(diff1_mean-diff2_mean) = {abs(diff1_mean-diff2_mean)}")

### adata_full normalization and log

# scvi uses non normalized data, so we save raw data first, do normalization/log, then take back the raw data, before proceeding with scvi
print("adata_full normalization and log...")


adata_full.layers["counts"] = adata_full.X.copy()
adata_full.raw = adata_full  # keep full dimension safe

sc.pp.normalize_total(adata_full, target_sum=1e4)
sc.pp.log1p(adata_full)

### vae 
print("vae...")

sc.pp.highly_variable_genes(
    adata_full,
    flavor="seurat_v3",
    n_top_genes=2000,
    layer="counts",
    batch_key="batch",
    subset=True
)

# number of originally filled features which ended up in the highly variable genes of the analysis
print(f"len(adata_full.var_names.intersection(common_genes)) = {len(adata_full.var_names.intersection(common_genes))}")

adata_full.raw = adata_full

scvi.model.SCVI.setup_anndata(adata_full, layer="counts", batch_key="batch")

num_latent = 30
vae = scvi.model.SCVI(adata_full, n_layers=2, n_latent=num_latent)

vae.train()

adata_full.obsm["X_scVI"] = vae.get_latent_representation()
sc.pp.neighbors(adata_full, use_rep="X_scVI")
sc.tl.leiden(adata_full)

adata_full.obsm["X_mde"] = mde(adata_full.obsm["X_scVI"])
sc.pl.embedding(
    adata_full,
    basis="X_mde",
    color=["batch", "leiden"],
    frameon=False,
    ncols=1,
)

print("adata")
print(adata)

# adata.layers['counts'] = adata.X.copy()
# sc.pp.highly_variable_genes(
#     adata,
#     flavor="seurat_v3",
#     n_top_genes=2000,
#     layer="counts",
#     batch_key="batch",
#     subset=True
# )

adata.var_names.intersection(common_genes)
scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")

num_latent = 30
vae = scvi.model.SCVI(adata, n_layers=2, n_latent=num_latent)
vae.train()

adata.obsm["X_scVI"] = vae.get_latent_representation()
sc.pp.neighbors(adata, use_rep="X_scVI")
sc.tl.leiden(adata)
adata.obsm["X_mde"] = mde(adata.obsm["X_scVI"])
sc.pl.embedding(
    adata,
    basis="X_mde",
    color=["batch", "leiden"],
    frameon=False,
    ncols=1,
)

### integration scoring
print("Integration scoring...")

ratio = len(adata_raw[0])/(len(adata_raw[1])+len(adata_raw[1]))
clusterwise_goodness = []
for i in set(adata_full.obs['leiden']):
  ct_d1 = 0
  ct_d2 = 0
  
  for j in adata_full[adata_full.obs['leiden'] == i].obs['batch']:
    if j == '0': ct_d1 += 1
    if j == '1': ct_d2 += 1
  clusterwise_goodness.append(abs(ratio-(ct_d1/(ct_d2+ct_d1)))*(ct_d1+ct_d2)/len(adata_full.obs_names))

print(f"score = {sum(clusterwise_goodness)/len(clusterwise_goodness)}")

ratio = len(adata_raw[0])/(len(adata_raw[1])+len(adata_raw[1]))
clusterwise_goodness = []
for i in set(adata.obs['leiden']):
  ct_d1 = 0
  ct_d2 = 0
  
  for j in adata[adata.obs['leiden'] == i].obs['batch']:
    if j == '0': ct_d1 += 1
    if j == '1': ct_d2 += 1
  clusterwise_goodness.append(abs(ratio-(ct_d1/(ct_d2+ct_d1)))*(ct_d1+ct_d2)/len(adata_full.obs_names))

print(f"score = {sum(clusterwise_goodness)/len(clusterwise_goodness)}")
